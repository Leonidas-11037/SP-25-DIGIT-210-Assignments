{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49f20452-8c0b-43b8-aa06-c7fbded88a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9c12940-38c7-4857-b76b-ba7a941a5845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('clear.n.01'),\n",
       " Synset('open.n.01'),\n",
       " Synset('unclutter.v.01'),\n",
       " Synset('clear.v.02'),\n",
       " Synset('clear_up.v.04'),\n",
       " Synset('authorize.v.01'),\n",
       " Synset('clear.v.05'),\n",
       " Synset('pass.v.09'),\n",
       " Synset('clear.v.07'),\n",
       " Synset('clear.v.08'),\n",
       " Synset('clear.v.09'),\n",
       " Synset('clear.v.10'),\n",
       " Synset('clear.v.11'),\n",
       " Synset('clear.v.12'),\n",
       " Synset('net.v.02'),\n",
       " Synset('net.v.01'),\n",
       " Synset('gain.v.08'),\n",
       " Synset('clear.v.16'),\n",
       " Synset('clear.v.17'),\n",
       " Synset('acquit.v.01'),\n",
       " Synset('clear.v.19'),\n",
       " Synset('clear.v.20'),\n",
       " Synset('clear.v.21'),\n",
       " Synset('clear.v.22'),\n",
       " Synset('clear.v.23'),\n",
       " Synset('clear.v.24'),\n",
       " Synset('clear.a.01'),\n",
       " Synset('clear.s.02'),\n",
       " Synset('clear.s.03'),\n",
       " Synset('clear.a.04'),\n",
       " Synset('clear.s.05'),\n",
       " Synset('clear.s.06'),\n",
       " Synset('clean.s.03'),\n",
       " Synset('clear.s.08'),\n",
       " Synset('clear.s.09'),\n",
       " Synset('well-defined.a.02'),\n",
       " Synset('clear.a.11'),\n",
       " Synset('clean.s.02'),\n",
       " Synset('clear.s.13'),\n",
       " Synset('clear.s.14'),\n",
       " Synset('clear.s.15'),\n",
       " Synset('absolved.s.01'),\n",
       " Synset('clear.s.17'),\n",
       " Synset('clear.r.01'),\n",
       " Synset('clearly.r.04')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SMOKE TEST: Explore Wordnet for specific words.\n",
    "wn.synsets('clear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e91fb6c-e90f-4e2e-9bcb-a0eb4b47bec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LEOHK\\Documents\\GitHub-Home\\SP-25-DIGIT-210-Assignments\\python-nlp\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a86b1f45-1ec7-40f9-843c-4b5b29bfd105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.idea',\n",
       " '.ipynb_checkpoints',\n",
       " '.venv',\n",
       " 'Ex4',\n",
       " 'Ex4.ipynb',\n",
       " 'grimm.txt',\n",
       " 'homework-notebook-sp25.ipynb',\n",
       " 'hughes-txt',\n",
       " 'nlp.ipynb',\n",
       " 'nlp.py',\n",
       " 'nlpexperiment.py',\n",
       " 'outputLinkText.txt',\n",
       " 'PersonalSiteScraper.py',\n",
       " 'read.py',\n",
       " '__pycache__']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remember, we defined cwd as our current working directory holding this file.\n",
    "# list directories:\n",
    "os.listdir(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e50423d-18fd-4be2-a9b0-01bde75110ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints', 'vol-4.txt', 'vol-5.txt', 'vol-6.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll = os.path.join(cwd, 'Ex4')\n",
    "os.listdir(coll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95fff058-a9d3-47bc-8513-4ac13d8b905f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LEOHK\\Documents\\GitHub-Home\\SP-25-DIGIT-210-Assignments\\python-nlp\\Ex4/vol-4.txt\n",
      "C:\\Users\\LEOHK\\Documents\\GitHub-Home\\SP-25-DIGIT-210-Assignments\\python-nlp\\Ex4/vol-5.txt\n",
      "C:\\Users\\LEOHK\\Documents\\GitHub-Home\\SP-25-DIGIT-210-Assignments\\python-nlp\\Ex4/vol-6.txt\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(coll):\n",
    "   if file.endswith(\".txt\"):\n",
    "        filepath = f\"{coll}/{file}\"\n",
    "        print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "876afce4-7456-4241-981d-6c28937cce70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints/vol-4-checkpoint.txt',\n",
       " 'vol-4.txt',\n",
       " 'vol-5.txt',\n",
       " 'vol-6.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = 'Ex4'\n",
    "corpus = PlaintextCorpusReader(corpus_root, '.*')\n",
    "corpus.fileids()\n",
    "# Check on one file in the collection\n",
    "# corpus.words('breakfast.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a36e598c-9e81-4f7d-b5bb-bf6aec2a0ed0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'python-nlp/Ex4/Vol-4.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtag\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pos_tag\n\u001b[32m      5\u001b[39m filepath = \u001b[33m'\u001b[39m\u001b[33mpython-nlp/Ex4/Vol-4.txt\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m f = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m.read()\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Make a list of tokens in your text. \u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# tokenList = f.split()\u001b[39;00m\n\u001b[32m      9\u001b[39m tokenList = word_tokenize(f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\GitHub-Home\\SP-25-DIGIT-210-Assignments\\python-nlp\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:325\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    320\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'python-nlp/Ex4/Vol-4.txt'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "filepath = 'python-nlp/Ex4/Vol-4.txt'\n",
    "f = open(filepath, 'r', encoding='utf8').read()\n",
    "# Make a list of tokens in your text. \n",
    "# tokenList = f.split()\n",
    "tokenList = word_tokenize(f)\n",
    "print(tokenList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8440fb6-e095-4890-baef-505fc4a37be4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
